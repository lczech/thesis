% ######################################################################################################################
%         Foundations
% ######################################################################################################################

\chapter{Foundations}
\label{ch:Foundations}

\paperbox{
    This chapter is partially based on the peer-reviewed publications:
}{\paperart \paperpppp}{
    \todo{
        \textbf{Contributions:} Lucas Czech... Pierre Barbera... Alexandros Stamatakis... and...
    }
}

\todo{In this chapter, we introduce\ldots}

% ######################################################################################################################
%         Evolution and Genetics
% ######################################################################################################################

\section{Evolution and Genetics}
\label{ch:Foundations:sec:EvolutionGenetics}

% Evolution is change in the heritable characteristics of biological populations over successive generations.
% \url{https://en.wikipedia.org/wiki/Evolution}
% the process by which new species or populations of living things develop from preexisting forms through successive generations
% \url{https://www.merriam-webster.com/dictionary/evolution}
% Evolution is a process of continuous branching and diversification from common trunks. This pattern of irreversible separation gives life's history its basic directionality. —Stephen Jay Gould
% \url{https://www.merriam-webster.com/dictionary/evolution}

% Evolution is the continuous process of diversification of biological populations through successive generations \cite{Hall2008}.

Life on Earth is at least 3.77 billion years old \cite{Dodd2017},
and is continuously evolving due to \emph{natural selection} \cite{Darwin1859}.
% Woah, I like that very first sentences. A very old and a very new publication, binding all of reasearch together...
Driven by \emph{variation}, biological populations diversify through successive generations,
leading to the origination of new species.
This continuous process is called \emph{evolution} \cite{Hall2008}.
Heritable characteristics are passed down from parent to offspring,
with occasional random mutations leading to variation.
Thus, some organisms are better adapted to their environment than others,
and have more reproductive success.
There is hence a natural selection for advantageous mutations,
which can then spread through generations.

% this process was not understood for a long time. static species?
% four driving forces of pop gen?

The characteristics and traits of an organism are carried by, and inherited via, \emph{deoxyribonucleic acid} (DNA).
DNA is the molecule that encodes the genetic information needed for the functioning of all living organisms.
It is structured in form of a double helix \cite{Watson1953},
and built from two strands of molecules called \emph{nucleotides}.
The nucleotides build the backbone of the double helix,
and connect the two strands via opposing pairs of \emph{nucleobases}, see \figref{fig:dna_nucleobases:sub:dna_helix}.
The redundant structure of pairs of nucleobases gives stability to the DNA molecule,
and also serves as a mechanism of error correction when reading the genetic information during cellular processes.
% both of which is important for the role of DNA for information storage.
In DNA, there are four distinct nucleobases:
adenine (\nucleobase{A}), cytosine (\nucleobase{C}), guanine (\nucleobase{G}), and thymine (\nucleobase{T}),
where \nucleobase{A} pairs with \nucleobase{T}, and \nucleobase{C} pairs with \nucleobase{G}, respectively,
as shown in \figref{fig:dna_nucleobases:sub:nucleobases}.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=.7\linewidth]{dna_nucleobases.pdf}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:dna_nucleobases:sub:dna_helix}
    \end{subfigure}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:dna_nucleobases:sub:nucleobases}
    \end{subfigure}
%     \begin{subfigure}{0pt}
%         \phantomcaption
%         \label{fig:dna_nucleobases:sub:transition_transversion}
%     \end{subfigure}
    \caption[DNA double helix and nucleobases]{
        \textbf{DNA double helix and nucleobases.}
        \subref{fig:dna_nucleobases:sub:dna_helix}
        The double helix structure of DNA, with the backbone in gray,
        connected by pairs of nucleobases.
        \subref{fig:dna_nucleobases:sub:nucleobases}
        The atomic structure of the four nucleobases, and their connection to each other.
%         Note that the diameter of the helix is about \SI{2}{\nano\meter}.
        Source and license: see \cite{Czech2018DNA},
        image derived from \cite{MesserWoland2006,Sponk2010,Yikrazuul2008a,Yikrazuul2008b}.
%         \\
%         Source: Image derived from \cite{Yikrazuul2008a,Yikrazuul2008b}.
%         Source: Image derived from \cite{Sponk2010}.
%         \subref{fig:dna_nucleobases:sub:transition_transversion}
%         Each nucleobase can mutate into any other in the course of evolution.
%         The two different types of mutation (transitions vs. transversions) are not equally likely,
%         due to physical properties of the nucleobases.
    }
    \label{fig:dna_nucleobases}
\end{figure}

% Transitions vs Transversions - not needed
% Over the course of evolutionary times, mutations in the DNA occur, mainly due to errors in the replication process.
% These mutations manifest in form of changed nucleobases.
% There are two types of mutations, \emph{transition} \emph{transversions}, which are not equally likely.
% \cite{Futuyma2013}
% \figref{fig:dna_nucleobases:sub:transition_transversion}

The sequence of nucleobases along the strands of DNA is what encodes the genetic instructions
used by all known living organisms.
Parts of the DNA encode for proteins,
which perform a plethora of different functions within organisms.
Proteins consist of long chains of amino acid residues, and
are assembled in a process called protein (bio)synthesis.
This is described by the central dogma of molecular biology \cite{Crick1958,Crick1970}:
First, DNA is \emph{transcribed} into the intermediary ribonucleic acid (RNA),
which is then \emph{translated} into the actual protein.

In each step of this process, the molecular alphabet used to encode information differs.
While DNA uses the four nucleobases as described above,
in RNA, the nucleobase uracil (\nucleobase{U}) is used instead of thymine (\nucleobase{T}).
Proteins on the other hand are (mostly) build from a set of \num{20} standard amino acids.
The set of rules used by the molecular machinery for translating nucleobases into amino acids
is called the \emph{genetic code}:
In a DNA sequence, three consecutive nucleobases are needed to encode one amino acid \cite{Shu2017}.

The entirety of the genetic material of an organism, that is, its complete DNA sequence, is called its \emph{genome}.
A \emph{gene} is a sequence which codes for a molecule that has a particular function, such as a protein \cite{Gericke2007}.
DNA and genes are the basic units of heredity.
They are what is varying across generations,
and what is selected for in the process of natural selection \cite{Dawkins1989}.
The study of genes, variation and heredity is called \emph{genetics} \cite{Griffiths2000}.

% ######################################################################################################################
%         Sequence Analysis
% ######################################################################################################################

\section{Sequence Analysis}
\label{ch:Foundations:sec:SequenceAnalysis}

All life on this planet is related to each other and descends from a common ancestor.
Still, it is remarkable that the basic molecular principles and mechanisms of life---%
DNA, amino acids, and the genetic code---are virtually identical for all living organisms.
This implies that by understanding and comparing the genetic information
encoded in the genetic sequences of different organisms,
one can understand the diversification patterns of evolution.

% ======================================================================================================================
%     Genome Sequencing
% ======================================================================================================================

\subsection{Genome Sequencing}
\label{ch:Foundations:sec:SequenceAnalysis:sub:GenomeSequencing}

Prior to analyzing the DNA of an organism, the physical order of nucleobases in the DNA molecule has to be determined.
That is, the DNA has to be ``read'' and stored in a human-accessible text format, typically a computer file.
This technical process is called DNA \emph{sequencing}.
% which can be understood as putting biomass into a blender and converting it into text files ;-)

For many decades, the main technique for this purpose was Sanger sequencing \cite{Sanger1975,Sanger1977}.
It is labor- and time-intensive, but through improvement and automation, costs were constantly reduced.
Eventually, this allowed for large-scale efforts, such as the Human Genome Project \cite{Venter2001},
which sequenced the whole human genome of more than three billion nucleobases.
Sanger sequencing allows to determine long parts of the sequence at once (> \num{500} nucleobases),
which then have to be assembled to build the final sequence.

In the last decades, a variety of novel high-throughput DNA sequencing technologiesß
have been developed \cite{Pettersson2009,Reuter2015,Goodwin2016}.
In particular, \emph{Next Generation Sequencing} (NGS) \cite{Logares2012,Mardis2013}
has revolutionized biology by transforming it into a data-driven and compute-intense discipline \citep{Escobar-Zepeda2015}.
The costs of these technologies are decreasing faster than Moore's law \cite{Wetterstrand2018}.
This leads to a ``tsunami'' of sequence data,
which poses a challenge for computational methods working with these data.
Compared to Sanger sequencing, NGS technologies are generally cheaper and faster \cite{Voelkerding2009,Metzker2010},
but come at the price of introducing more errors in the sequence output,
or only being able to determine shorter parts of the sequence at once
-- both of which constitute a challenge for the subsequent assembly of the final sequence.

The result of DNA sequencing is a textual representation of the order of nucleobases.
Although this representation ignores the physical and chemical properties of the respective molecules,
it is helpful in many applications, and allows to leverage existing algorithms.
Each contiguous sequence coming from the sequencing machine is called a \emph{read}.
Because of the pairing of nucleobases,
both DNA strands can be sequenced, which provides a means of error correction.
Such data are typically stored in the \fileformat{fastq} file format \citep{Cock2009}.
These so-called paired-end reads are then merged to form a final sequence representation of one strand
% cite \toolname{PEAR} \cite{Zhang2014} ?
-- that is, a sequence of the characters \nucleobase{A}, \nucleobase{C}, \nucleobase{G}, and \nucleobase{T}.
These data are stored in formats such as the \fileformat{fasta} file format \citep{Pearson1988}.
Due to the pairing of nucleobases,
the length of a DNA sequence is measured in \emph{base pairs} (abbreviated \si{\basepair}):
\SI{1}{\basepair} represents one character in the file.
These files are then the input for computational methods for working with DNA sequences.

% Not needed right now:
% Because of the universal genetic code of translating DNA to amino acids,
% it is also possible to store the amino acid sequence instead of the nucleobase sequence.

% ======================================================================================================================
%     Metagenomics
% ======================================================================================================================

\subsection{Metagenomics}
\label{ch:Foundations:sec:SequenceAnalysis:sub:Metagenomics}

Sanger sequencing requires careful preparation of the genetic material,
and is thus best used for sequencing single organisms.
There are however many (microbial) organisms that cannot be cultured in a Petri dish,
and are hence hard to sequence with this technique.
Apart from being cheaper, Next Generation Sequencing machines however ``digest'' all genetic material presented to them.
They thus allow for studying microbial samples
directly extracted from their environment \citep{Morgan2010,Edwards2013,Sunagawa2013a}.
This enables to study environments such as
water \cite{Karsenti2011,Giner2016,Gran-Stadniczenko2017},
soil \cite{Dupont2016,Mahe2017},
the human body \cite{Huttenhower2012,Methe2012,Matsen2015,Wang2015},
and many others.
Each sample from such an environment then represents a geographical location, a body site, a point in time, etc.
% These studies yield a large set of short anonymous DNA reads for each sample.
% The DNA reads obtained from sequencing a sample represent all organisms present in the sample;
The DNA of all organisms being present in a sample is sequenced,
resulting in a large number of reads per sample.
These reads are anonymous, as it is unclear to which organism they originally belonged.
The study of these data, that is, genetic material from environmental samples, is called \emph{metagenomics} \cite{Oulas2015}.

% \paragraph{Key Tasks}
% \label{ch:Foundations:sec:SequenceAnalysis:sub:Metagenomics:par:KeyTasks}

% A first step is often to generate a genetic profile of the environment,
% that is, to estimate the diversity of the organisms in the sample.
A first step in metagenomic studies is often to characterize the reads obtained from an environment
in terms of \emph{reference sequences} of known species.
Reads that are similar to (parts of) reference sequences can be assigned to them,
while reads with low similarity to known sequences might indicate novel, undescribed species \cite{Temperton2012}.
Key tasks in metagenomic studies are the identification and classification of the anonymous reads (``Who is there?''),
and their functional annotation (``What are they doing?'') \cite{Desai2012}.
Both are introduced in the following.

% The following paragraph is not totally needed, as we are not doing functional analysis.
% However, it introduces many other terms alongside, which is useful later on.
Functional annotation \cite{Stein2001} is the prediction of gene functions of the reads,
and the inference of metabolic capacity of microbial communities \cite{Brown2017}.
As the proteins that are needed in the pathways of such functions can be encoded by genes across the genome,
whole-genome sequencing is necessary to capture all genes of interest.
For example, in shotgun sequencing \cite{Staden1979,Anderson1981},
the DNA is fragmented into small pieces within the size range that the used sequencing technology can handle,
typically a few hundred \si{\basepair} long.
This allows to sequence all genetic material contained in a sample.
Thus, the resulting reads originate from different parts of the genomes of their organisms,
which can then be functionally annotated \cite{Glass2010}.
This however necessitates to use whole-genome reference sequences
in order to be able to assign reads to known species and functions.
Typical databases of reference sequences however
lack many of the protein sequences from the microbial species present in a sample,
mostly because of organisms that cannot be cultured \cite{Brown2017}.

For the task of identification and classification of reads however, whole genome references are not needed.
Instead, specific \emph{marker genes} can be used,
which are regions of genes that are particularly suited for delineating between different species \cite{Ren2016}.
% highly conserved, that is, slowly changing over evolutionary times.
% but certain parts are radpidly evolung (hyper variable) withing the region, which is good for delineation.
% typically, this is general basic cell functionen stuff, which is needed by many forms of live.
% eg cell division, ssu... mitochondrial dna, etc
The method of using marker genes to identify species is called \emph{DNA barcoding} \cite{Hebert2003,Savolainen2005,Deiner2017b}.
The choice of genes to use as marker is important, and depends on the types of organisms to be studied.
The used marker genes should ideally be present in all of the organisms of interest,
short enough to be sequenced with current technology,
and have enough variation between species to distinguish between them,
but have low variation within species \cite{Kress2008}.

% \paragraph{Typical Processing}
% \label{ch:Foundations:sec:SequenceAnalysis:sub:Metagenomics:par:TypicalProcessing}

In many metagenomic studies of \taxonname{bacteria} and \taxonname{eukaryota},
the 16S \cite{Weisburg1991} and 18S \cite{Meyer2010} rRNA regions are used as marker genes,
respectively \cite{Woese1977,Woese1990}.
These regions belong to the small subunit (SSU) of the ribosomal ribonucleic acid (rRNA),
which is an essential component of the ribosome.
The ribosome is a molecular machinery that is responsible for protein synthesis (translation) in all living organisms.
% and are thus present in virtually all \taxonname{bacteria} and \taxonname{eukaryota}.
Often, prior to sequencing, these regions are amplified by many orders of magnitude,
using polymerase chain reaction (PCR) to create copies of these regions \cite{Bartlett2003}.
The resulting reads are then de-replicated again, which results in sequences called \emph{amplicons}.
While the PCR amplification process is known to introduce bias \cite{Logares2014,Brown2017},
this inexpensive method is commonly used in practice, particularly for the 16S and 18S rDNA regions.
A recent alternative to using PCR for obtaining reads from these regions are \textsubscript{mi}tags \citep{Logares2014}.
In this approach, shotgun sequencing is used to get reads from the whole genomes of the organisms in a sample.
These reads are then filtered to only contain reads from the 16S region (for \taxonname{bacteria}),
which capture the diversity of the sample without bias.

Because of the ubiquity of the 16S and 18S rDNA regions in organisms and, consequently, in sequencing studies,
many databases provide reference sequences for these marker regions.
The reads or amplicons obtained from an environmental sample can then be employed
to estimate the microbial diversity of the organisms in the sample by comparison against known species.

% ======================================================================================================================
%     Sequence Alignments
% ======================================================================================================================

\subsection{Sequence Alignment}
\label{ch:Foundations:sec:SequenceAnalysis:sub:SequenceAlignment}

Organisms that evolved from a (not too distant) common ancestor share genetic information.
Regions of their DNA that have a shared ancestry are called \emph{homologous} regions \cite{Koonin2005}.
This homology is typically inferred from sequence similarity. %, that is, by comparison of the sequences.
However, due to mutations, differences in the sequences can occur.
There are three main types of sequence mutations that can occur in evolution:
a \emph{substitution} is the exchange of a nucleobase for another;
a \emph{insertion} adds one or more extra nucleotides into the sequence;
a \emph{deletion} removes one or more nucleotides from the sequence.
The latter two types of mutations change the length of the sequence;
a mutation that is either one of them is called an \emph{indel}.

Because of indels, sequences have to be aligned to each other in order to compare their homologous regions.
That is, gap characters (\nucleobase{-}) have to be added to the sequences
such that homologous characters in the sequence get aligned.
This results in an $n \times m$ matrix,
where $n$ is the number of sequences (rows),
and $m$ is the number of homologous characters (columns), called \emph{sites}.
This matrix is called a \emph{multiple sequence alignment} (MSA), or simply an \emph{alignment}.
\figref{fig:msa} shows an example of the alignment process and the resulting MSA.

\begin{figure}[hpbt]
    \centering
%     \vspace*{0.5em}
    \includegraphics[width=.9\linewidth]{msa.pdf}
    \caption[Multiple Sequence Alignment]{
        \textbf{Multiple Sequence Alignment.}
        The left hand side shows a set of six sequences.
        Using an alignment algorithm, gaps are inserted into these sequences at presumed indel positions.
        The right hand side shows the result of this process,
        where homologous characters at the sites of the multiple sequence alignment (MSA) are aligned to each other.
        Below the MSA, the majority rule consensus sequence is shown,
        see \secref{ch:Foundations:sec:SequenceAnalysis:sub:ConsensusSequences}.
    }
    \label{fig:msa}
\end{figure}

Sequence alignment can be understood as an optimization problem under a given optimality criterion.
On the one hand, \emph{global alignments} attempt to align every character in every sequence,
which is most useful for similar sequences of roughly equal size.
For example, the Needleman-Wunsch algorithm \cite{Needleman1970} is a general global alignment technique
based on dynamic programming.
On the other hand, \emph{local alignments} are better suited for dissimilar sequences
which might contain similar regions within a larger sequence context.
The Smith-Waterman algorithm \cite{Smith1981} is a general local alignment technique
using the same dynamic programming scheme,
which additionally allows to start and end at any place in the sequence.
As both algorithms have their particular use cases \cite{Polyanovsky2011},
hybrid methods have also been developed \cite{Brudno2003}.
Furthermore, heuristic approaches such as \toolname{BLAST} \cite{Altschul1990} and
\toolname{USEARCH} \cite{Edgar2010} can calculate millions of near-optimal alignments in reasonable time.

These algorithms are efficient for the pairwise alignment of two sequences.
Calculating an MSA however has been shown to be NP-hard \cite{Wang1994,Just2001}.
Thus, for most empirical data sets, other approaches and heuristics are needed \cite{Thompson2011}.
Tools such as \toolname{CLUSTAL} \cite{Higgins1988}, \toolname{MUSCLE} \cite{Edgar2004}, and \toolname{MAFFT} \cite{Katoh2002}
can calculate near-optimal multiple sequence alignments for many thousands of sequences.

A special use case for aligning sequences arises in metagenomic studies,
where environmental sequences are often compared to a set of known reference sequences.
In these studies, one often first calculates an MSA of the reference sequences,
and then successively aligns the environmental sequences against this MSA.
This is because calculating an MSA for millions or billions of sequences from scratch is too expensive even for modern tools.
Hence, specialized algorithms for this use case have been developed,
such as \toolname{PaPaRa} \citep{Berger2011a,Berger2012} and
\toolname{hmmalign}, which is a subprogram of the \toolname{HMMER} suite \citep{Eddy1998,Eddy2009}.

% ======================================================================================================================
%     Consensus Sequences
% ======================================================================================================================

\subsection{Consensus Sequences}
\label{ch:Foundations:sec:SequenceAnalysis:sub:ConsensusSequences}

When working with a number of related but not identical sequences,
it is often convenient to ``summarize'' homologous characters in form of a \emph{consensus sequence}.
Such a sequence is typically calculated based on the relative frequencies of the characters per alignment site.
It then represents typical features and motifs of the input set of sequences.

The most straight forward method is to construct \emph{majority rule consensus} sequences \citep{May1952,Day1992a},
where each site is represented by the most frequent character at that site.
\figref{fig:msa} shows an example below the MSA on the right hand side.
In order to also include information from the less frequent characters at a site in the consensus sequence,
\emph{ambiguity characters} can be used \cite{IUPAC1970}.
They allow to denote multiple alternative nucleobases as a single character.
For example, if the nucleobases \nucleobase{A} and \nucleobase{G} are similarly frequent at a site,
this site is represented by the ambiguity character \nucleobase{R}.
See \tabref{tab:AmbiguityCharacters} for the full list of character representations.

Using ambiguity characters allows for more involved consensus methods.
For example, \emph{threshold consensus} sequences \citep{Day1992a,Day1992} include the most frequent characters
that are needed to achieve some given frequency threshold per site,
and represent these characters by their ambiguity character.
Furthermore, many methods based on fixed thresholds have been proposed,
such as Cavener's method \citep{Cavener1987,Cavener1991a};
see \cite{Day1992a} for a critical comparison.

It is theoretically also possible to directly use the relative frequencies of characters per site
in the mathematical frameworks of many downstream analysis methods.
This would allow to leverage all of the information contained in the input set of sequences.
However, to our knowledge, there is no convention or file format to store such information,
and consequently, no way of forwarding this information to the respective tools.

% ######################################################################################################################
%         Tree of Life
% ######################################################################################################################

\section{The Tree of Life}
\label{ch:Foundations:sec:TreeOfLife}

The shared evolutionary history of life gives rise to a branching pattern,
where new \emph{lineages} split from a common ancestor.
This branching pattern forms a tree-like structure,
which classifies organisms in a hierarchy based on common descent.

While this \emph{tree of life} is an expedient and, hence, pervasive model \cite{Mindell2013},
it ignores certain biological and evolutionary events.
A strict hierarchy does not allow for reticulate events,
such as hybridization \cite{Maddison1997a}, genetic recombination \cite{Hein1993},
or horizontal gene transfer \cite{Ochman2000,DunningHotopp2011,Robinson2013}.
Although approaches such as networks have been proposed to model these events \cite{Huson2011a},
the simplicity of a hierarchy or tree structure still has proven
to be useful in classifying and naming organisms, and understanding their evolutionary relationships.

% not considering lat gene tranfs, there is only one ``true'' tree of life.

% For the \taxonname{eukaryota}, the tree model is still considered valid.
% However, \taxonname{bacteria} have the ability of horizontal gene transfer,
% where genetic information is transferred between unrelated organisms.

% biology is messy: https://www.ncbi.nlm.nih.gov/pubmed/20852602

% ======================================================================================================================
%     Taxonomy and Nomenclature
% ======================================================================================================================

\subsection{Taxonomy and Nomenclature}
\label{ch:Foundations:sec:TreeOfLife:sub:TaxonomyNomenclature}

Early attempts at classifying organisms date back to the Greek philosopher Aristotle,
who used observable attributes to divide living things into groups \cite{Leroi2014}.
This approach as well as the efforts of later centuries were non-uniform and inconsistent.
The basis for the modern system of classification was established
by Swedish botanist Carl Linnaeus in the mid-18th century \cite{Donk1957}.
He proposed a \emph{nomenclature}, that is, a naming system for organisms,
as well as a \emph{taxonomy}, that is, a rank-based classification of organisms \cite{Linnaeus1735,Linnaeus1753}.

A taxonomic group of organisms is called a \emph{taxon} (plural: \emph{taxa}).
Each taxon is associated with a \emph{taxonomic rank}, which can subsume other ranks,
thus forming a hierarchy of higher and lower ranks.
A taxonomic rank represents the relative level of a group of organisms in the taxonomy.
The principal ranks in modern use are \taxonname{domain}, \taxonname{kingdom}, \taxonname{phylum}, \taxonname{class},
\taxonname{order}, \taxonname{family}, \taxonname{genus}, and \taxonname{species},
see \figref{fig:taxonomic_ranks}.
If needed, further ranks can be included in between (such as \emph{sub-genus}),
or more refined lower levels be added (such as \emph{strain}, which is a further distinction within a species).

\begin{figure}[hpbt]
    \centering
%     \vspace*{0.5em}
    \includegraphics[width=.85\linewidth]{taxonomic_ranks.pdf}
    \caption[Biological classification into taxonomic ranks]{
        \textbf{Biological classification into taxonomic ranks.}
        The figure depicts a typical set of nested taxonomic ranks \cite{Woese1990},
        which form a hierarchy with increasingly deeper levels towards the right.
%         The taxonomic ranks shown here follow the three-domain system \cite{Woese1990}.
%         which classifies cellular life into the domains \taxonname{archaea}, \taxonname{eukaryota}, and \taxonname{bacteria}.
%         The deeper levels towards the right form a hierarchy of nested taxonomic ranks.
        Source: Image derived from \cite{Halasz2007}.
    }
    \label{fig:taxonomic_ranks}
\end{figure}

In order to scientifically name the groups of organisms (taxa) in a taxonomy,
the \emph{binomial nomenclature} as introduced by Linnaeus is still prevalent to this day.
It uses two terms, often of Latin origin,
which respectively specify the taxonomic ranks \emph{genus} and \emph{species} that an organism belongs to,
for example \taxonname{Homo sapiens}.

While early classifications used \emph{phenotypes}, that is, observable characteristics or traits of organisms,
modern approaches to taxonomy take genetic information into account \cite{Mayr2002}.
For example, the three-domain system \cite{Woese1977,Woese1990} resolves the oldest evolutionary relationships,
that is, the highest taxonomic levels, based on 16S rRNA data.
Although this classification has been challenged \cite{Gupta1998,Mayr1998,Cavalier-Smith2002}, it is widely used.
It divides cellular life forms into the three domains \taxonname{bacteria}, \taxonname{archaea}, and \taxonname{eukaryota};
the latter is further separated into kingdoms,
which include the kingdoms \taxonname{fungi}, \taxonname{plants}, and \taxonname{animals}.

% Archaea, Domain (and Kingdom)
% Eukarya, Domain
% Protista, Kingdom
% Fungi, Kingdom
% Animalia, Kingdom
% Plantae, Kingdom
% Bacteria, Domain (and Kingdom)

% ======================================================================================================================
%     Phylogenetic Trees
% ======================================================================================================================

\subsection{Phylogenetic Trees}
\label{ch:Foundations:sec:TreeOfLife:sub:PhylogeneticTrees}

The classification of organisms into a taxonomy is based on (subjective) dissimilarity and thus arbitrary:
The number of organisms that are grouped into a taxon at a given rank can vary,
and the separation into discrete ranks does not reflect the gradual nature of evolution \cite{Gingerich1987}.
A more involved approach that can resolve these issues is \emph{phylogenetics},
which is the study of the evolutionary history and relationships of biological entities (individuals, species, populations).

The evolutionary relationships of such entities are called their \emph{phylogeny}.
As the true phylogeny of a set of taxa is unknowable,
it has to be inferred from data that is available.
A phylogenetic analysis uses inference methods that evaluate observed heritable traits
in order to resolve the phylogeny under a given model of evolution of these traits.
While phenotypes can be used, modern phylogenetic inference is mostly based on DNA data.
The result of a phylogenetic analysis is a \emph{phylogenetic tree},
also called an \emph{evolutionary tree}, or---synonymously---a \emph{phylogeny}.
\figref{fig:tree_of_life} shows two examples of phylogenetic trees.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=\linewidth]{tree_of_life.pdf}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:tree_of_life:sub:darwin}
    \end{subfigure}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:tree_of_life:sub:woese}
    \end{subfigure}
    \caption[Exemplary phylogenetic trees]{
        \textbf{Exemplary phylogenetic trees.}
        \subref{fig:tree_of_life:sub:darwin}
        In 1837, Charles Darwin sketched his first evolutionary tree below the words ``I think''
        in his notebook on ``Transmutation of Species''.
        Source: Image derived from \cite{DarwinTreeOfLife1837}.
%         \\
        \subref{fig:tree_of_life:sub:woese}
        A modern phylogenetic tree showing the three-domain system \cite{Woese1977,Woese1990},
        which emphasizes the separation of \taxonname{bacteria}, \taxonname{archaea}, and \taxonname{eukaryota}
        based on 16S rRNA genes.
        The black branch at the bottom represents the speculative last universal common ancestor of all living organisms.
        Source: Image derived from \cite{WoeseTreeOfLife2006}.
    }
    \label{fig:tree_of_life}
\end{figure}

\paragraph{Properties of Trees}
\label{ch:Foundations:sec:TreeOfLife:sub:PhylogeneticTrees:par:TreeProperties}

The leaf nodes, or \emph{tips}, of the tree represent living (\emph{extant}) biological entities
such as species, strains, individual organisms, or even cells of a multicellular organism.
Thus, the tips are often referred to as the \emph{taxa} of the tree,
which is meant as a generic term that includes all the above entities.
The tips are often named according to the entities they represent (e.\,g., species names);
such a tree is called a \emph{labeled} tree.
The inner nodes on the other hand are usually anonymous and
represent speciation events, where two novel lineages arose from a putative common ancestor.
The branching pattern of a phylogenetic tree thus reveals the evolutionary history of its taxa.

The edges of the tree, also called its \emph{branches}, can have associated \emph{branch lengths},
which represent the evolutionary time between the two adjacent nodes,
for example, measured as the average change in nucleobases between their respective sequences.
The unique path between any two nodes thus can be interpreted
as a measure of evolutionary relatedness of the taxa represented by the nodes.

A phylogenetic tree is \emph{rooted}
if it is a directed tree that has a unique \emph{root node},
which corresponds to the putative common ancestor of the other nodes in the tree.
See \figref{fig:tree_of_life:sub:woese} for an example.
As evolution is a processes that happens over time,
from a biological point of view, every tree has a root.
However, most models of DNA evolution are time-reversible,
meaning that the direction of change in the sequences cannot be inferred from the data under such models,
see \secref{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol} for details.
Thus, tree inference methods can also yield \emph{unrooted} trees without direction and without a root node.
In these methods, for computational reasons, often a \emph{virtual root} is used,
which is a hypothetical additional node placed on a branch of the tree.
For tasks such as traversing a tree, but also in order to store a tree in a file,
unrooted trees usually have a distinguished, but arbitrary, ``starting'' node called a \emph{top-level trifurcation}.
An unrooted tree can be rooted a posteriori, for example by using an \emph{outgroup} of taxa
that are closely related to the group of taxa of interest (the \emph{ingroup}), but not part of it.
Then, a root node can be placed on the branch that separates the outgroup from the ingroup.

An inner node that has exactly three neighboring nodes is called a \emph{bifurcation} or a \emph{bifurcating} node.
In rooted trees, these neighbors are the the parent and the two children of a node, hence the name.
An inner node with more neighbors is called a \emph{multifurcation} or a \emph{multifurcating} node.
This naming also applies to the whole tree:
A tree, where each inner node (with the exception of the root node in a rooted tree) is bifurcating,
is also called a bifurcating tree.
Otherwise (if there is at least one multifurcating node), it is a multifurcating tree.
Note that in evolution, an actual multifurcation event is highly unlikely,
as it corresponds to the simultaneous formation of more than two new lineages from a single ancestral lineage.
Multifurcating trees are for example used when relationships cannot be properly resolved from the existing data,
or to summarize a set of otherwise contradicting trees.

Each edge of the tree induces a \emph{bipartition} or \emph{split} of the taxa of the tree into two groups,
one on each side of the edge.
A set of taxa is \emph{monophyletic},
if there is a bipartition of the tree that splits these taxa from all other taxa of the tree.
In a rooted tree, the node at the end of that edge is then the putative common ancestor of these taxa.
Furthermore, a monophyletic set of taxa is called a \emph{clade} of the tree;
in other words, a clade is a subtree that is separated from the rest of the tree by one edge.
For example, the taxa {\sffamily A}, {\sffamily B}, and {\sffamily C} in \figref{fig:tree_vis_options} are monophyletic---%
that is, they form a clade of the tree.
Lastly, a non-monophyletic set of taxa is \emph{paraphyletic}.

\paragraph{Practical Aspects of Trees}
\label{ch:Foundations:sec:TreeOfLife:sub:PhylogeneticTrees:par:PracticalAspects}

While the topology of the tree is what models the evolutionary relationships,
there are several ways of visualizing or drawing that information.
\figref{fig:tree_vis_options} shows some examples, which all visualize the same topology (except for the rooting).
The figure also summarizes some of the terms and concepts introduced above.
Different drawing styles each have their advantages.
For example, in a rectangular tree (\figref{fig:tree_vis_options:sub:rectangular}),
branch lengths are easier to read and compare,
while a circular tree (\figref{fig:tree_vis_options:sub:circular}) can fit more taxa in the same drawing area.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=\linewidth]{tree_vis_options.pdf}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:tree_vis_options:sub:unrooted}
    \end{subfigure}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:tree_vis_options:sub:rectangular}
    \end{subfigure}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:tree_vis_options:sub:circular}
    \end{subfigure}
    \caption[Types of phylogenetic trees]{
        \textbf{Types of phylogenetic trees.}
        Here, we show three different types of labeled, bifurcating trees.
        Tip nodes are marked with black dots, inner nodes with white dots,
        and the top-level trifurcation or root node with a larger green dot.
        \subref{fig:tree_vis_options:sub:unrooted}
        An unrooted tree with five taxa. One node is arbitrarily selected as top-level trifurcation.
        \subref{fig:tree_vis_options:sub:rectangular}
        The same tree topology,
        but rooted on the inner branch that splits the taxa {\sffamily D} and {\sffamily E} from the other taxa.
        The tree is drawn in rectangular style,
        where vertical lines correspond to branch lengths.
        The horizontal lines are simply used to distribute the taxa, and have no biological interpretation.
        \subref{fig:tree_vis_options:sub:circular}
        The same tree again, but this time drawn in circular style.
        Here, radial lines correspond to branch lengths, while arcs only serve drawing purposes.
    }
    \label{fig:tree_vis_options}
\end{figure}

Taxonomy and phylogeny serve a related, but different purpose:
While the former is a system of classification,
the latter reveals evolutionary history.
However, there is a correspondence between a taxonomy and a rooted phylogeny:
Inner nodes of the tree constitute older evolutionary relationships,
which are represented by the higher ranks of the taxonomy.
\figref{fig:tree_of_life:sub:woese} shows such a correspondence for the three domains of life.
It is however possible that the taxa at one rank of the taxonomy are not monophyletic in the tree.
In this case, the two are \emph{incongruent}.

% particularly considering that the genes might evolve differently from the species or from what the taxonomy says

The most common file format for storing phylogenetic trees is the \fileformat{Newick} format \cite{Archie1986},
which uses parentheses and commas to specify the nesting structure of the tree,
and allows to store node labels and branch lengths.
The \fileformat{NEXUS} format \cite{Maddison1997} is a container format for biological data,
and internally also relies on the \fileformat{Newick} format for storing trees.
Furthermore, the \fileformat{phyloXML} format \cite{Han2009} is an \fileformat{XML} based format
that allows to store arbitrary data at the nodes and edges of the tree.
% This is particularly useful for annotating the tree with additional data.

% ======================================================================================================================
%         Tree Inference
% ======================================================================================================================

\subsection{Tree Inference}
\label{ch:Foundations:sec:TreeOfLife:sub:TreeInference}

A phylogeny can be inferred from data that has per-taxon traits which are homologous,
that is, which have evolved from the same traits in the common ancestor and are thus comparable \cite{Felsenstein2004,Yang2006}.
While historically these traits were mostly phenotypes (bone shapes and sizes, metabolism, etc.),
the focus has since shifted towards molecular data such as DNA and amino acid sequences,
as their \emph{phylogenetic signal} is generally more abundant and less biased \cite{Hillis2000}.
Most often, a multiple sequence alignment is used,
whose homologous sites represent the traits of the taxa.
To determine the degree of relatedness between taxa, mathematical models of trait evolution are employed.

% Searching the tree space
The general concept of tree inference is then to put closely related taxa close to each other in the phylogeny.
Hence, a tree inference can be thought of as an optimization problem,
which searches for the best tree given an optimality criterion.
However, the space of all possible tree topologies is too large for an exhaustive brute-force search
for virtually all empirical datasets.
For a given number of taxa $n$, the number of distinct tree topologies N is given as
$N(n) = \prod_{\,i=3}^{\,n} ~(2i - 5)$,
which grows over-exponentially fast \cite{Felsenstein2004}.
There are thus different approaches and heuristics to conduct tree search.

% Methods overview
Distance based methods such as \emph{Unweighted Pair Group Method with Arithmetic Mean} (UPGMA) \cite{Sokal1958}
and \emph{Neighbor Joining} \cite{Saitou1987}
use a pairwise distance matrix between sequences, and thus do not necessarily need an alignment.
\emph{Maximum Parsimony} \cite{Sankoff1975} uses an optimality criterion that is based on Occam's razor,
that is, it yields the tree that explains the observed tip sequences (taxa)
with the minimal number of substitutions (mutations).
The \emph{Maximum Likelihood} (ML) method \cite{Felsenstein1981} employs statistical techniques
in order to evaluate the probability of a given phylogenetic tree with respect to a given alignment,
and successively search the tree space for the most likely tree, see \secref{ch:Foundations:sec:MLTreeInference}.
Furthermore, \emph{Bayesian Inference} also relies on the evaluation of tree probability \cite{Yang2006},
and uses Bayes' theorem to calculate the posterior distribution of the relevant evolutionary processes;
it thus can incorporate prior empirical knowledge into the process.

Typical software tools for inferring ML trees include \toolname{IQ-TREE} \cite{Nguyen2015a},
\toolname{FastTree} \cite{Price2010}, and \toolname{RAxML} \cite{Stamatakis2014},
while Bayesian inference can for example be conduced using tools such as \toolname{BEAST} \cite{Suchard2018}
or \toolname{MrBayes} \cite{Ronquist2003}.

% ######################################################################################################################
%     Maximum Likelihood Tree Inference
% ######################################################################################################################

\section{Maximum Likelihood Tree Inference}
\label{ch:Foundations:sec:MLTreeInference}

In the context of this work, we are mostly interested in Maximum Likelihood (ML) tree inference.
It uses a probabilistic framework in which the (phylogenetic) likelihood

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:likelihood}
    \mathcal{L}(~ \mbox{MSA} ~|~ T, \bar{b}, M, \bar{\theta} ~)
\end{equation}

is evaluated that an observed MSA is the outcome of an evolutionary history described by a phylogenetic tree
with topology $T$ and branch lengths $\bar{b}$, under a given model of trait evolution $M$ with parameters $\bar{\theta}$.
For a fixed model $M$ (see \secref{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol}),
the likelihood can be expressed as a function of $T$, $\bar{b}$ and $\bar{\theta}$,
which is known as the \emph{phylogenetic likelihood function} (PLF).

% ======================================================================================================================
%     Tree Search
% ======================================================================================================================

\subsection{Tree Search}
\label{ch:Foundations:sec:MLTreeInference:sub:TreeSearch}

By maximizing the PLF using maximum likelihood estimation,
the parameter values (including tree topology) are found which best explain the observed data.
This process is called \emph{tree search}.
Typically, the estimates are obtained in an iterative process,
which alternates between two phases until a (potentially local) optimum is found:

\begin{enumerate}
    \item Optimizing the tree topology $T$, given the branch lengths $\bar{b}$ and the model parameters $\bar{\theta}$.
    \item Optimizing the branch lengths of the given tree topology, as well as the model parameters.
\end{enumerate}

% Optimizing the topology:
Finding the most likely tree topology is a discrete optimization problem,
which has been shown to be NP-hard under the ML criterion \cite{Chor2005}.
Furthermore, the evaluation of the PLF is computationally expensive,
as it involves many floating point operations, see \secref{ch:Foundations:sec:MLTreeInference:sub:LikelihoodComputation}.
A general heuristic for the tree search that avoids an exhaustive evaluation of the tree space is thus as follows.
First, a starting tree is generated, either randomly, or using methods such as Neighbor Joining or Maximum Parsimony.
Then, the likelihood of the tree is successively improved
by applying topological rearrangements (\emph{moves}) of its taxa and clades.
For instance, in \emph{greedy hill-climbing} \cite{Stamatakis2014}, only those moves are applied (\emph{accepted})
that immediately improve the PLF.

%Optimizing branch lengths and model parameters:
For a fixed tree topology $T$, the maximum likelihood estimates
of the branch lengths $\bar{b}$ and the model parameters $\bar{\theta}$
are usually obtained with general-purpose numerical optimization methods.
Since the derivatives of the PLF can be easily computed,
the Newton-Raphson method \cite{Ypma1995} is often used for optimizing the branch lengths,
see \secref{ch:Foundations:sec:MLTreeInference:sub:BLO}.
Model parameters are commonly optimized with Brent's method \cite{Brent1971}
or with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm \cite{Fletcher1987}.

% ======================================================================================================================
%     Models of Molecular Sequence Evolution
% ======================================================================================================================

\subsection{Models of Molecular Sequence Evolution}
\label{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol}

So far, we assumed to have a model $M$ for describing the evolution of the traits that are used for inferring the tree.
For sequence data, such a model yields an estimate of the evolutionary distance between the sequences of different taxa.
Because the inference assumes homologous traits,
the only mutations that are typically considered in aligned sequences are substitutions.
% Gap characters in the MSA (see \secref{ch:Foundations:sec:SequenceAnalysis:sub:SequenceAlignment}),
% which account for indels, can then be thought of as another type of substitution.

\paragraph{Markov Chain Model}
\label{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol:par:MCModel}

Most commonly, a continuous-time Markov chain (MC) is used to describe
the evolution of a single site within a set of aligned sequences \cite{Gagniuc2017}.
For DNA data, the MC has four states \nucleobase{A}, \nucleobase{C}, \nucleobase{G}, and \nucleobase{T},
which correspond to the nucleobases, and transitions between the states correspond to their substitutions,
see \figref{fig:mc_state_transitions}.
While the MC model ignores aspects such as natural selection and the molecular mechanisms of evolution,
it describes the relative rate of changes in a way that allows multiple substitutions to occur
along the same branch (\nucleobase{T} $\rightarrow$ \nucleobase{A} $\rightarrow$ \nucleobase{G}).
% \todo{if transition vs transversion is explained above, we need to clarify here:
% note that transition is used in two different meanings here...}

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=.35\linewidth]{mc_state_transitions.pdf}
    \caption[Markov chain model of nucleotide substitutions]{
        \textbf{Markov chain model of nucleotide substitutions.}
        The evolution of characters at a site in an alignment can be modeled as a Markov chain (MC).
        The states of the MC for DNA data are the four nucleobases
        \nucleobase{A}, \nucleobase{C}, \nucleobase{G}, and \nucleobase{T}.
        The model allows transitions with rates $q_{ij}$ with $i,j \in \left\{~ A, C, G, T ~\right\}, i \neq j$ between all states,
        which correspond to substitutions of the nucleobases.
    }
    \label{fig:mc_state_transitions}
\end{figure}

% \begin{figure}[hpbt]
%  \begin{subfigure}[c]{.5\textwidth}
%     \centering
%     \includegraphics[width=.7\linewidth]{mc_state_transitions.pdf}
%     \caption{}
%     \label{fig:mc_state_transitions:sub:mc_chain}
%  \end{subfigure}
%  \begin{subfigure}[c]{.5\textwidth}
%     \begin{align*}
%         Q ~&=~
%          \begin{pmatrix}
%          ~-q_{A}   &   q_{AC}   &   q_{AG}   &   q_{AT}~~ \\
%          ~q_{CA}   &  -q_{C}    &   q_{CG}   &   q_{CT}~~ \\
%          ~q_{GA}   &   q_{GC}   &  -q_{G}    &   q_{GT}~~ \\
%          ~q_{TA}   &   q_{TC}   &   q_{TG}   &  -q_{T}~~
%          \end{pmatrix}
%         \\ \\
%         - q_{i} ~&=~ - \sum_{j \neq i} q_{ij}, ~ i = \left\{~ A, C, G, T ~\right\}
%     \end{align*}
%     \caption{}
%     \label{fig:mc_state_transitions:sub:matrix}
%   \end{subfigure}
%     \caption[Markov chain model of nucleotide substitutions]{
%         \textbf{Markov chain Model of Nucleotide Substitutions.}
%         The evolution of characters at a site in an alignment can be modeled as a Markov chain (MC).
%         \subref{fig:mc_state_transitions:sub:mc_chain} The states of the MC for DNA data are
%         the four nucleobases \nucleobase{A}, \nucleobase{C}, \nucleobase{G}, and \nucleobase{T}.
%         The model allows transitions between all states, which correspond to substitutions between the nucleobases.
%         \subref{fig:mc_state_transitions:sub:matrix} The instantaneous transition rates between states are given
%         by the substitution rate matrix (Q-matrix), where the rows must sum to $0$.
%     }
%     \label{fig:mc_state_transitions}
% \end{figure}

The process of state transitions is defined by the substitution rate matrix %$Q$

\begin{equation}
    \begin{align*}
        Q ~&=~
         \begin{pmatrix}
         ~-q_{A}   &   q_{AC}   &   q_{AG}   &   q_{AT}~~ \\
         ~q_{CA}   &  -q_{C}    &   q_{CG}   &   q_{CT}~~ \\
         ~q_{GA}   &   q_{GC}   &  -q_{G}    &   q_{GT}~~ \\
         ~q_{TA}   &   q_{TC}   &   q_{TG}   &  -q_{T}~~
         \end{pmatrix}
        \\ \\
        - q_{i} ~&=~ - \sum_{j \neq i} q_{ij}, ~ i \in \left\{~ A, C, G, T ~\right\}
    \end{align*}
\end{equation}

% The instantaneous transition rates between states are given
% by the substitution rate matrix (Q-matrix), where the rows must sum to $0$.
where the elements $q_{ij}$ are the \emph{instantaneous transition rates} from state $i$ to state $j$.
The rows of the $Q$-matrix have the requirement to sum to $0$,
by which the diagonal elements $q_{i}$ are defined.

The expected number of substitutions at an alignment site between two nodes of the tree
is expressed as the branch length $b$ between the nodes, and is a measure of evolutionary time $t$ between them.
Under the MC model, evolutionary time and branch length are proportional to each other
with the \emph{evolutionary rate} $r$ being their proportionality factor: $t = r \cdot b$.
Then, for a given time $t$, the \emph{transition probabilities} $p_{ij}(t)$ between states in a stationary process
are obtained by exponentiating the $Q$-matrix \cite{Yang2014}.
These probabilities are specified by the matrix

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:P_matrix}
    P(t) = e^{Qt}
\end{equation}

For positive transition rates $q_{ij} > 0, \forall i \neq j$, if the process runs long enough,
the Markov chain eventually reaches the unique \emph{stationary} distribution $\Pi = (\pi_A$, $\pi_C$, $\pi_G$, $\pi_T )$,
with $\pi_i$ being the proportion of time spent in state $i$.
If the Markov process reached equilibrium after running long enough,
it can be interpreted as having generated the sequences of the MSA.
In that case, $\Pi$ is the \emph{equilibrium base composition} of the MSA,
and $\pi_i$ are the \emph{equilibrium} or \emph{stationary base frequencies} of the MSA.

\paragraph{Time-Reversible Models}
\label{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol:par:TimeReversibleModels}

As mentioned before, most models of DNA evolution assume a \emph{time-reversible} Markov process,
which means that $\pi_{i} q_{ij} = \pi_{j} q_{ji} ~ \forall \, i \neq j$.
This assumption is biologically not meaningful, as evolution is a process in time, and thus does have a direction.
It however allows for simplified calculations:
The $Q$-matrix of a time-reversible model can be formulated as the product of
a symmetric rate matrix $R = \left\{ r_{i \leftrightarrow j} \right\}$ and
a diagonal matrix with the stationary base frequencies:

\begin{align}
    \newcommand{\lra}{\leftrightarrow}
    Q = R \cdot diag(\pi_i) =
    \begin{pmatrix}
         -q_A                       &   r_{A \lra C} \cdot \pi_C   &   r_{A \lra G} \cdot \pi_G   &   r_{A \lra T} \cdot \pi_T~~  \\
        ~r_{A \lra C} \cdot \pi_A   &   -q_C                       &   r_{C \lra G} \cdot \pi_G   &   r_{C \lra T} \cdot \pi_T~~  \\
        ~r_{A \lra G} \cdot \pi_A   &   r_{C \lra G} \cdot \pi_C   &   -q_G                       &   r_{G \lra T} \cdot \pi_T~~  \\
        ~r_{A \lra T} \cdot \pi_A   &   r_{C \lra T} \cdot \pi_C   &   r_{G \lra T} \cdot \pi_G   &   -q_T
    \end{pmatrix}
\end{align}

The matrix describes the most general model of DNA evolution,
where all \num{6} substitution rates $r_{i \leftrightarrow j}, i \neq j$ and
all \num{4} base frequencies $\pi_i$ can be different.
This model is called the Generalized Time-Reversible (\toolname{GTR}) model \cite{Tavare1986}.
As the sum of the base frequencies must be $1$,
and as the substitution rates are usually normalized by requiring that $r_{G \leftrightarrow T} = 1.0$,
the \toolname{GTR} model has a total of \num{8} free parameters
(that is, \num{3} base frequencies, and \num{5} substitution rates).
The base frequencies can also be estimated from the character frequencies in the given MSA,
in which case they are called the \emph{empirical} base frequencies.

There are also more restrictive models, which have fewer free parameters,
and are thus more robust if data for estimating them is sparse, at the expense of descriptiveness.
The Jukes-Cantor model (\toolname{JC69}) \cite{Jukes1969} has no free parameter
and assumes equal substitution rates $r_{i \leftrightarrow j} = 1$, $i \neq j$
and equal base frequencies $\pi_i = \sfrac{1}{4}$.
The \toolname{K80} model \cite{Kimura1980} adds a free parameter $\kappa$,
which describes the ratio between two types of substitutions that are not equally likely to occur in evolution:
$r_{A \leftrightarrow C} = r_{G \leftrightarrow T} = \kappa \cdot r_{A \leftrightarrow G} =
\kappa \cdot r_{A \leftrightarrow T} = \kappa \cdot r_{C \leftrightarrow G} = \kappa \cdot r_{C \leftrightarrow T}$.
% \todo{if transition vs transversion was added earlier, add it here, too!}
The \toolname{F81} model \cite{Felsenstein1981} instead extends the JC69
by allowing different base frequencies: $\pi_A \neq \pi_C \neq \pi_G \neq \pi_T$.
The \toolname{HKY85} model \cite{Hasegawa1985} combines the \toolname{K80} model and the \toolname{F81} model,
and hence has \num{4} free parameters.
Further models have also been proposed, which offer compromises
between the number of free parameters and the expressiveness of the model \cite{Yang2014}.

The state space of the Markov process becomes significantly larger for protein data,
as it needs to comprise \num{20} standard amino acids instead of \num{4} nucleobases.
Hence, the \toolname{GTR} model for protein data has $(400 - 20) / 2 - 1 + 19 = 208$ free parameters.
Typical amino acid alignments do not contain enough data to reliably estimate these parameters,
and thus easily lead to over-fitting.
Thus, so-called \emph{empirical} amino acid models are commonly used,
which have substitution rates and equilibrium base frequencies
that were pre-estimated on large collections of reference alignments.
Among others, some popular models include \toolname{DAYHOFF} \cite{Dayhoff1978},
\toolname{WAG} \cite{Whelan2001}, and \toolname{LG} \cite{Le2008}.

% ======================================================================================================================
%     Further Aspects of Tree Inference
% ======================================================================================================================

\subsection{Further Aspects of Tree Inference}
\label{ch:Foundations:sec:MLTreeInference:sub:FurtherAspects}

Evolution is an incredibly complex process with intricate details.
Many more models and methods have thus been proposed to refine tree inference \cite{Yang2014},
of which we here briefly introduce a few.
% For understanding the methods described in this work, the above introduction should suffice.
% For the distinguished reader, we here still mention a few other aspects that are related to the topic.
% While a detailed description of these models and methods is not necessary for understanding the main chapters of this work,
% we here still mention a few other related aspects.

\paragraph{Rate Heterogeneity}
\label{ch:Foundations:sec:MLTreeInference:sub:FurtherAspects:par:RateHeterogeneity}

The models of sequence evolution described above make the simplifying assumption
that the alignment sites evolve independently and are identically distributed.
However, certain regions of DNA or amino acid sequences are under higher evolutionary pressure than others,
for example if they describe important molecular functions that are conserved in their evolutionary history.
It is thus expected that some alignment sites evolve faster than others.
That is, the evolutionary rate $r$ of these sites is not constant across the alignment.
In the context of phylogenetic inference,
several models of \emph{rate heterogeneity among sites} have been proposed to account for this,
some of which are described in the following.

A simple model is the \emph{proportion of invariable sites},
where the likelihood of an alignment site is influenced by a parameter $p \in [0,1]$
that describes the proportion of sites that are assumed to be identical (\emph{invariable}) across all taxa.
The more elaborate $\Gamma$ model \cite{Yang1994a} postulates a shape parameter $\alpha > 0$
which models the rate heterogeneity as a gamma distribution $\Gamma(\alpha)$.
The distribution shape ranges from exponential-like ($\alpha < 1$, high rate heterogeneity)
to normal-like ($\alpha > 10$, low rate heterogeneity).
Thus, by optimizing the single free parameter $\alpha$,
different unimodal rate heterogeneity profiles can be approximated.
The \toolname{CAT} or \emph{per-site rates} model \cite{Stamatakis2006a}
is a compute- and memory-efficient approximation of the $\Gamma$ model,
which explicitly assigns one of $K$ rate categories to each alignment site instead of using a distribution of rates.
Lastly, the \toolname{FreeRate} model \cite{Yang1995} allows for multimodal distributions
by using $K$ rate categories and respective weights,
which can approximate any distribution at the cost of having to estimate these free parameters.

\paragraph{Alignment Partitioning}
\label{ch:Foundations:sec:MLTreeInference:sub:FurtherAspects:par:AlignmentPartitioning}

Apart from the evolutionary rate $r$, also the substitution patterns among the sites of an MSA can differ.
In order to account for this, the MSA can be split into different \emph{partitions},
where each such partition is assigned its own model of evolution.
For example, as three nucleobases code for one amino acid in regions that encode for proteins
(see \secref{ch:Foundations:sec:EvolutionGenetics}),
three partition can be used, each modeling the evolution of the first, second, and third nucleobase of each amino acid.
Furthermore, large multi-gene MSAs can be split into partitions corresponding to individual genes,
which might be under different evolutionary pressure each.

\paragraph{Constrained Trees}
\label{ch:Foundations:sec:MLTreeInference:sub:FurtherAspects:par:ConstrainedTrees}

The tree search (see \secref{ch:Foundations:sec:MLTreeInference:sub:TreeSearch})
can (theoretically) yield any topology from the vast space of possible trees.
It is however often serviceable to run a \emph{constrained} tree search,
for example to include prior knowledge about the taxa,
to maintain congruence with a given taxonomy, or because some other constraints are required.
Such a constraint can for example be specified by enforcing certain bipartitions to be retained in the tree,
that is, splits of the taxa that must be separated from each other in the tree.
As a bipartition is induced by a branch in the tree,
this is equivalent to starting the tree search with a multifurcating tree,
and then resolving these multifurcations without changing the other parts of the tree.
A constrained search yields a \emph{constrained tree}.

% Not needed:
% Alignment Partitioning

% ======================================================================================================================
%     Likelihood Computation
% ======================================================================================================================

\subsection{Likelihood Computation}
\label{ch:Foundations:sec:MLTreeInference:sub:LikelihoodComputation}

We here introduce the basic computational aspects of the Maximum Likelihood method.
For a more exhaustive description of the topic, see \cite{Yang2014}.
We assume a fixed tree topology $T$, fix branch lengths $\bar{b}$,
as well as a given model of sequence evolution $M$ with parameters $\bar{\theta}$.
That is, we do not cover the tree search itself here,
% \secref{ch:Foundations:sec:MLTreeInference:sub:TreeSearch}
but describe how to compute the likelihood $\mathcal{L}$
(\eqnref{ch:Foundations:sec:MLTreeInference:eq:likelihood}) for a given MSA under these conditions.

A central point of the ML method is to account for the unknown states at the inner nodes of the tree.
That is, the total likelihood is obtained by summing over the probabilities of every possible state of the inner nodes,
which can be efficiently computed by the \emph{Felsenstein pruning algorithm} (FPA) \cite{Felsenstein1981}.
It traverses the tree in post-order fashion, that is from the tips towards the (virtual) root,
and recursively calculates a so-called \emph{conditional likelihood vector} (CLV) at each inner node.

In a sense, a CLV summarizes the subtree below its corresponding node.
For every alignment site and every state,
it describes the \emph{conditional likelihood} of the node to be in that state at that site,
given the subtree topology and its branch lengths, for the respective subset of the alignment (tip sequences).
We here assume a set $N$ of states, that is, the sequences consist of characters $c \in N$,
for example $N = \left\{~ \nucleobase{A}, \nucleobase{C}, \nucleobase{G}, \nucleobase{T} ~\right\}$.
Furthermore, for simplicity, we do not consider alignment partitioning or rate heterogeneity among sites here,
and thus use a fixed evolutionary rate $r$.
Then, a CLV contains $|N|$ elements per alignment site,
each describing the conditional likelihood of being in the corresponding state.
For DNA data, these are CL(\nucleobase{A}), CL(\nucleobase{C}), CL(\nucleobase{G}), and CL(\nucleobase{T}).

\paragraph{Felsenstein Pruning Algorithm}
\label{ch:Foundations:sec:MLTreeInference:sub:LikelihoodComputations:par:FPA}

In order to start the recursion of the FPA, first, the CLVs at the tip nodes have to be initialized.
In principle, these can be the actual likelihoods of observing the characters $c \in N$ at the corresponding site.
However, this uncertainty is rarely available in empirical data.
Thus, tip nodes are usually initialized with ``pseudo-CLVs'',
where for instance a nucleobase \nucleobase{A} in the alignment yields
CL(\nucleobase{A}) $= 1$, and CL(\nucleobase{C}) $=$ CL(\nucleobase{G}) $=$ CL(\nucleobase{T}) $= 0$.

\begin{figure}[pthb]
    \centering
    \includegraphics[width=.85\linewidth]{clv_fpa.pdf}
    \caption[Felsenstein pruning algorithm]{
        \textbf{Felsenstein pruning algorithm.}
        An exemplary tree topology with a (virtual) root {\sffamily R}, an inner node {\sffamily V},
        and three other nodes {\sffamily U}, {\sffamily L}, and {\sffamily M},
%         (which can be either tips or inner nodes),
        and branch lengths between nodes.
        Potential subtrees are marked by triangles below nodes.
%         For the recursive computation during Felsenstein's pruning,
%         Because of the recursive nature of the algorithm,
%         it is irrelevant whether the last three nodes are tips or have subtrees (marked here by triangles below them).
        \\
        Each node has a CLV assigned to it, which ``summarizes'' the subtree below it.
        A CLV stores a conditional likelihood for every alignment site and for every state.
        Here, for simplicity, we show the CLVs for one site and for four states,
        which for instance represent the likelihood of that site to be in either state of the four nucleobases.
        %\nucleobase{A}, \nucleobase{C}, \nucleobase{G}, and \nucleobase{T}.
%         Lastly, the branches of the tree have branch lengths assigned to them.
    }
    \label{fig:clv_fpa}
\end{figure}

After the tip CLVs have been initialized, the algorithm traverses up the tree, see \figref{fig:clv_fpa}.
This can be thought of as moving along the branches towards a parent node,
which induces the possibility of state transitions.
This step is thus where the model $M$ is employed (see \secref{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol}).
As we assumed a fixed evolutionary rate $r$,
we can infer the time $t$ between two nodes from the branch length $b$ between them: $t = r \cdot b$.
% Then, in order to compute the likelihood of state changes while moving from a child node to its parent,
% the model $M$ is employed, as explained in \secref{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol}.
% In particular,
Then, for a given branch, we can compute the probability $p_{ij}(t)$ of a transition
from state $i$ to state $j$ after moving along the branch, see \eqnref{ch:Foundations:sec:MLTreeInference:eq:P_matrix}.
Note that the probability $p_{ij}$ depends on the branch length,
meaning that for every branch (and every update in its branch length during optimization,
see \secref{ch:Foundations:sec:MLTreeInference:sub:BLO}),
a separate $P$-matrix has to be computed from the $Q$-matrix of the model.

At a parent node whose children have been computed, we can now apply the recursion step of the algorithm.
For instance, in the topology shown in \figref{fig:clv_fpa}, the CLV of an inner node {\sffamily V}
can be computed given its children {\sffamily L} and {\sffamily M}.
For the computation, the CLVs of the two child nodes,
as well as the transition probabilities $p_{ij}$ for the branch lengths $b_{LV}$ and $b_{MV}$
of the branches towards the parent are needed.
% and the respective branch lengths $b_{LV}$ and $b_{MV}$.
Then, a single entry of the CLV, that is,
the conditional likelihood of node {\sffamily V} to be in state $c$ at site $s$, is

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:CLV}
%     \mbox{CLV} =
    \mbox{CLV}^{(V)}_{s,c} =
    \left(~ \sum_{j \in N} p_{cj}(r \cdot b_{LV}) \cdot \mbox{CLV}^{(L)}_{s,j} \right)
    \left(~ \sum_{k \in N} p_{ck}(r \cdot b_{MV}) \cdot \mbox{CLV}^{(M)}_{s,k} \right)
\end{equation}

The equation can be interpreted as follows:
The product of transition probability and conditional likelihood represents a change from state $c$ to another state in $N$.
By summing this product for all states, all possible inner states are accounted for.
Finally, the product of the two sums is the new conditional likelihood of the node being in state $c$ at site $s$,
given the evolutionary history of its children and their subtrees.

By repeating the computation for every state $c \in N$ and every site $s$ of the alignment,
the complete CLV for node {\sffamily V} is computed.
The recursion is then applied to all nodes upwards the tree, until all CLVs are computed.

\paragraph{Likelihood Evaluation at the Root}
\label{ch:Foundations:sec:MLTreeInference:sub:LikelihoodComputations:par:RootLikelihood}

Once all CLVs are computed, the overall likelihood $\mathcal{L}$ can be computed from the CLV of the root node.
Given the root node {\sffamily R} as shown in \figref{fig:clv_fpa},
the total \emph{per-site likelihood} $\mathcal{L}_s$ of an alignment site $s$ is
accumulated from the conditional likelihoods of all states,
taking their respective base frequencies $\pi_i$ into account:

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:root_site_likelihood}
    \mathcal{L}_{s} = \sum_{i \in N} \pi_i \cdot \mbox{CLV}^{(R)}_{s,i}
\end{equation}

Due to the time reversibility of the model, for unrooted trees, a \emph{virtual} root can be used,
that is, an additional node that is presumed to be present on a branch of the tree.
% The calculation is invariant to the exact position of this node along the branch.
% which might coincide with an actual node of the tree
If for example the node {\sffamily R} in \figref{fig:clv_fpa} represents a virtual root,
the two branches between nodes {\sffamily U} and {\sffamily V}
are actually one branch with branch length $b_{UV} = b_{UR} + b_{VR}$.
Then, an alternative way of computing the per-site likelihood is what we here call the (per-site) \emph{edge likelihood}.
Instead of using the CLV at the virtual root {\sffamily R}, we can use the CLVs of {\sffamily U} and {\sffamily V},
and the corresponding branch length $b_{UV}$ for the computation.
In that case, state transitions along the branch have to be additionally accounted for in the computation.
The per-site edge likelihood of an alignment site $s$ can then be computed as

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:edge_likelihood}
    \mathcal{L}_{s} = \sum_{i \in N} \sum_{j \in N} \pi_i \cdot \mbox{CLV}^{(U)}_{s,i} \cdot
    p_{ij}(r \cdot b_{UV}) \cdot \mbox{CLV}^{(V)}_{s,j}
\end{equation}

This way of calculating the edge likelihood $\mathcal{L}_s$ works for any two adjacent nodes,
if their respective CLVs have been computed to represent the two subtrees induced by the branch between the nodes.

Finally, the overall likelihood $\mathcal{L}( \mbox{MSA} ~|~ T, \bar{b}, M, \bar{\theta} )$
for the entire MSA can be computed.
For mathematical simplicity, the sites are generally assumed to evolve independently,
although this is not expected to be the case from a biological perspective.
Then, for an alignment with $m$ sites, the overall likelihood is simply the product of the per-site or edge likelihoods:

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:root_likelihood}
    \mathcal{L} = \prod_{s=1}^{m} \mathcal{L}_s
\end{equation}

For computational reasons, and to avoid numerical underflow, in practice,
the logarithm of the likelihood (\emph{log-likelihood}) is typically used.
Furthermore, as identical sites yield exactly the same likelihood,
such sites are often compressed and the respective likelihood is multiplied with an according \emph{weight}.
% Lastly, we note that the derivatives of the above likelihood functions can be obtained analytically \cite{Yang2014},
% which is needed for optimizing the branch lengths,
% as explained in \secref{ch:Foundations:sec:MLTreeInference:sub:TreeSearch}.

% ======================================================================================================================
%     Branch Length Optimization
% ======================================================================================================================

\subsection{Branch Length Optimization}
\label{ch:Foundations:sec:MLTreeInference:sub:BLO}

Another important aspect of the tree search
is the optimization of the branch lengths of the tree.
That is, for a given tree topology $T$, and a fixed model $M$ with parameters $\bar{\theta}$,
we want to compute the branch lengths $\bar{b}$
that maximize the likelihood $\mathcal{L}( \mbox{MSA} ~|~ T, \bar{b}, M, \bar{\theta} )$.
This procedure is called \emph{branch length optimization} (BLO),
and typically uses the Newton-Raphson method \cite{Ypma1995},
as mentioned in \secref{ch:Foundations:sec:MLTreeInference:sub:TreeSearch}.

We consider to optimize a single branch length $b$.
In order to maximize $\mathcal{L}$, we need to find the root of the first derivative $\mathcal{L}'$.
The Newton-Raphson method takes an initial value for $b$ and then
iteratively approximates values that lead closer to the root:

\begin{equation}
    \label{ch:Foundations:sec:MLTreeInference:eq:BLO}
    b_{n+1} = b_n - \frac{ \mathcal{L}' }{ \mathcal{L}'' }
\end{equation}

Note that the derivatives $\mathcal{L}'$ and $\mathcal{L}''$ can be obtained analytically \cite{Yang2014},
and have to be computed in every iteration.
The algorithm stops when the change in $b$ between two iterations is below a threshold,
that is, when then optimization \emph{converges}.
This procedure is repeated for all branch lengths $\bar{b}$ in the tree.

% ######################################################################################################################
%         Phylogenetic Placement
% ######################################################################################################################

\section{Phylogenetic Placement}
\label{ch:Foundations:sec:PhylogeneticPlacement}

In studies of sequence data, one of the most common tasks is a phylogenetic analysis of the data,
that is, to infer the evolutionary context of the sequences.
However, since the amount of sequence data produced in typical metagenomic studies can be quite substantial,
computational challenges and bottlenecks arise \cite{Scholz2012}.
In particular, both calculating an MSA and inferring a phylogeny are NP-hard \cite{Just2001,Chor2005},
and thus impractical or infeasible for large datasets.
Furthermore, metagenomic reads are often short, and hence lack phylogenetic signal
to robustly infer a tree and to properly resolve their relationships.

Thus, \emph{phylogenetic placement}, also called \emph{evolutionary placement},
has been developed for conducting phylogenetic analyses of such data,
as implemented in tools such as
\toolname{pplacer} \cite{Matsen2010}, \toolname{RAxML-EPA} \cite{Berger2011}, and \toolname{EPA-ng} \cite{Barbera2018}.
% cited more than 630 times (as of 2018-07-01)
Instead of resolving the phylogeny of a set of metagenomic sequences,
phylogenetic placement treats each sequence, called a \emph{\acl{QS}}\acused{QS} (\acs{QS}), separately.
It evaluates how these \acp{QS} relate to an existing \emph{\acl{RT}}\acused{RT} (\acs{RT}) based on known reference sequences.
For each \ac{QS}, it computes the probabilities of \emph{placing} the sequence on the branches of the \ac{RT},
% In other words, it maps the \acp{QS} of interest onto the branches of an \ac{RT},
thereby classifying them into a phylogenetic context of related sequences,
without the need to resolve relationships between the \acp{QS}.

Phylogenetic placement can be carried out if the \acp{QS} can be aligned to the alignment of the reference sequences.
In the most common use case, the \acp{QS} are reads or amplicons from environmental samples.
Most often barcoding regions or marker genes such as 16S or 18S are used
(see \secref{ch:Foundations:sec:SequenceAnalysis:sub:Metagenomics}),
but there also exist studies that use different, or even a set of, maker genes \citep{Sunagawa2013a}.
Furthermore, other types of sequences such as \textsubscript{mi}tags \citep{Logares2014} can be used.

The \ac{RT} and the reference sequences it represents are typically assembled by the user
so that they capture the expected species diversity in the samples.
We however proposed an automated approach for assembling suitable sets of reference sequences \cite{Czech2018},
which we describe in \chpref{ch:AutomaticTrees}.
Distinct samples from one study are typically placed on the same underlying \ac{RT}
in order to facilitate comparisons between the samples,
see \secref{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods}.
% Such a mapping also elucidates the evolutionary distance between the query and the reference sequences,
% see \secref{ch:Foundations:sec:PhylogeneticPlacementAnalysis:sub:Distances}.

% The result of phylogenetic placement is a mapping of the query sequences to the branches of the reference tree.
% In brief, phylogenetic placement calculates the most probable insertion branches for each given \acf{QS} on a \acf{RT}.

% In short, for a given \acf{QS}, phylogenetic placement computes a mapping of the \ac{QS}
% to the most closely related branches of a fixed \acf{RT} based on known reference sequences.
% Phylogenetic placement uses a fixed \acf{RT} based on known reference sequences to compute a mapping
% of \acfp{QS} to the branches of the tree that are most closely related to the \ac{QS}.
% This mapping classifies the \acp{QS} (e.g., sequences from a metagenomic study) into a phylogenetic context

% ======================================================================================================================
%     Pipeline and Computation
% ======================================================================================================================

\subsection{Pipeline and Computation}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PipelineAndComputation}

% \todo{pipeline diagram, data flow, etc, reference to chapters?!}

% \paragraph{Preparation}
% \label{ch:Foundations:sec:PhylogeneticPlacement:sub::par:Preparation}

We here assume to have given a set of suitable reference sequences, their alignment, and an \ac{RT} inferred from them.
As phylogenetic placement used a maximum likelihood criterion, the \ac{RT} has to be strictly bifurcating.
Prior to the placement, the \acp{QS} need to be aligned against the reference alignment of the \ac{RT}
by programs such as \toolname{PaPaRa} \cite{Berger2011a,Berger2012} or \toolname{hmmalign} \cite{Eddy1998,Eddy2009},
see also \secref{ch:Foundations:sec:SequenceAnalysis:sub:SequenceAlignment}.
The input to phylogenetic placement are then the \acf{RT}, its underlying alignment, and the aligned \acfp{QS}.

\paragraph{Computation for one Query Sequence}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PipelineAndComputation:par:ComputingQuerySequence}

% The placement is then conducted for each \ac{QS} separately
% by initially inserting a \ac{QS} as a new tip into a branch of the tree,
% then re-optimizing the branch lengths that are most affected by the insertion,
% and thereafter evaluating the resulting likelihood score of the tree.
% The \ac{QS} is then removed from the current branch and subsequently placed into all other branches of the \ac{RT}.

The placement is conducted for each \ac{QS} separately, always using the same fixed \ac{RT} as a starting point.
Each branch of the tree is evaluated as a potential \emph{placement location} of the \ac{QS},
which indicates how likely the branch is to be the ancestor of the \ac{QS}.
In \figref{fig:tiny_tree},
the procedure for one \ac{QS} and one branch (between nodes {\sffamily D} and {\sffamily P}) is shown:
The sequence is inserted as a new tip node {\sffamily Q} on the branch,
connected to it by a new \emph{pendant} branch and a new node {\sffamily C}.
This splits the original branch into two parts, called the \emph{distal} and the \emph{proximal} branch, respectively,
which are named according to the direction of the root of the tree.
Note that the tree can also be unrooted, in which case a top-level trifurcation is typically used as root.

\begin{figure}[pthb]
    \centering
    \includegraphics[width=.7\linewidth]{tiny_tree.pdf}
    \caption[Terminology of a phylogenetic placement]{
        \textbf{Terminology of a phylogenetic placement.}
        The nodes {\sffamily D} and {\sffamily P} belong to the reference tree (RT).
        When placing a query sequence (QS), the branch between them %{\sffamily D} and {\sffamily P}
        is split into two parts by a new node {\sffamily C},
        which serves as the attachment point for another new node {\sffamily Q} that represents the QS.
        The \emph{pendant} branch leads to {\sffamily Q}.
        The original branch is split into the \emph{proximal} branch, which leads towards the root of the RT,
        and the \emph{distal} branch, which leads away from the root.
    }
    \label{fig:tiny_tree}
\end{figure}

In the next step, the branch lengths of the tree are optimized,
as explained in \secref{ch:Foundations:sec:MLTreeInference:sub:BLO}.
After the optimization, the sum of the lengths of the distal and proximal branches is not necessarily equal
to the original branch length between {\sffamily D} and {\sffamily P}.
Thus, typically, the two lengths are proportionally rescaled to maintain this equality.

Lastly, the likelihood of the tree with the newly attached sequence is evaluated
as explained in \secref{ch:Foundations:sec:MLTreeInference:sub:LikelihoodComputation}.
Note that the likelihood computation uses the MSA (extended by the query sequence),
the tree topology and branch lengths, as well as the model and its parameters as before.
The model of nucleotide evolution should be the same that was used when inferring the tree,
see \secref{ch:Foundations:sec:MLTreeInference:sub:ModelsOfSeqEvol}.
% mention that epa ng can use all the model stuff explaine above: subst models, rate hetero, etc
After this, the newly created nodes on the branch are removed again, thus restoring the original reference tree.

The above procedure is repeated for every branch $i$ of the tree $T$,
yielding a set of likelihood scores $\mathcal{L}(i)$ for each possible placement location.
Thus, for each branch of the tree, the process yields a so-called \emph{placement} of the \ac{QS},
that is, an optimized position on the branch, along with a likelihood score for the whole tree.
The likelihood scores for a \ac{QS} are then transformed into probabilities,
which quantify the uncertainty of placing the sequence on the respective branch \cite{Strimmer2002,VonMering2007}.
The probability of placing a \ac{QS} on a branch $q$ is called the \emph{\acl{LWR}}\acused{LWR} (\acs{LWR}),
and is calculated relative to the other branches $i$ of the tree $T$ as

\begin{equation}
    \label{ch:Foundations:sec:PhylogeneticPlacement:eq:LWR}
    \mbox{LWR}(q) =  \frac{ \mathcal{L}( q )}{ \sum_{i \in T} \mathcal{L}( i )}
\end{equation}

By construction, the sum of the \acp{LWR} of all branches for a single \ac{QS} is $1.0$.
It can thus be interpreted as a probability distribution over the branches of the tree,
as shown in \figref{fig:tree_epa}.
For most use cases, the \acp{LWR} and the respective placement on the branches are the most important values,
while pendant lengths are rarely used in downstream analyses.
However, long pendant lengths can indicate that the \ac{RT} is missing sequences that are closely related to the \ac{QS}.

\begin{figure}[hpbt]
    \centering
    \includegraphics[width=.6\linewidth]{pppp/tree_epa.pdf}
    \caption[Phylogenetic Placement of a Query Sequence]{
        \textbf{Phylogenetic Placement of a Query Sequence.}
        Each branch of the reference tree is tested as a potential insertion position, called a \emph{placement}
        (blue dots; pendant lengths are ignored here).
        Note that placements have a specific position on their branch, due to the branch length optimization process.
        A probability of how likely it is that the sequence belongs to a specific branch is computed
        (numbers next to dots),
        which is called the \emph{likelihood weight ratio} (LWR).
%         based on the maximum likelihood score of the whole tree. %(that is, including the query sequence)
        The bold number (0.7) denotes the most probable placement of the sequence.
    }
    \label{fig:tree_epa}
\end{figure}

\paragraph{Accelerations}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PipelineAndComputation:par:Accelerations}

The most expensive part of the placement computation are the branch lengths optimizations.
Thus, several techniques have been developed to accelerate the placement process \cite{Barbera2018}.

Firstly, above, all branches of the tree were optimized when evaluation a placement location,
which gives the most accurate results.
In practice however, it suffices to only optimize the three branches of the placement location
without losing too much accuracy.

Secondly, because the reference tree is fixed (except for the newly created nodes),
the CLVs of all possible subtrees can be precomputed,
which drastically accelerates the likelihood evaluation.
Using \figref{fig:tiny_tree} as an example,
with two CLVs of the nodes {\sffamily D} and {\sffamily P}, and one application of the Felsenstein Pruning Algorithm
(see \secref{ch:Foundations:sec:MLTreeInference:sub:LikelihoodComputations:par:FPA}),
the CLV of node {\sffamily C} can be computed.
Then, the final likelihood can be evaluated as the edge likelihood of the pendant edge,
using this CLV as well as the pseudo-CLV of node {\sffamily Q}.

Thirdly, further acceleration can be achieved with a \emph{pre-placement} heuristic:
A first approximate evaluation of a placement location can be conducted by using
distal and proximal lengths based on the respective original branch length of the tree, and a fixed default pendant length.
Then, only the most likely fraction of locations is thoroughly evaluated, that is, including branch length optimizations.
This way, millions or even billions of sequences can be placed within acceptable time \cite{Barbera2018}.
This is however a heuristic that might lead to suboptimal results by ignoring placement locations
whose \ac{LWR} is significantly improved by the branch length optimization process.
If the \ac{RT} is however well suited for the \acp{QS}, this situation is generally not expected to occur.

\paragraph{Placement Result}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PipelineAndComputation:par:PlacementResults}

The placement process is repeated independently for every \ac{QS}.
That is, for each \ac{QS}, the algorithm starts calculating placements from scratch on the original \ac{RT}.
The result thus classifies each \ac{QS} in the phylogenetic context of the \ac{RT},
without resolving the evolutionary relationships between the \acp{QS}.

The data is usually stored in so-called \fileformat{jplace} files \cite{Matsen2012},
which is a standard based on the \fileformat{JSON} format \cite{JsonMemo,JsonStandard}.
It stores the \ac{RT} in \fileformat{Newick} format, including tip names and branch lengths,
and extended by a post-order numbering of the edges to be referenced by the placements.
The main part of the file is a list of lists:
For each \ac{QS}, its list of placements is stored.
A placement is described by its edge number (referencing the \ac{RT}),
the \ac{LWR}, the pendant length, and the distal length.
The proximal length is usually omitted, as it can be inferred from the branch length of the tree.
Furthermore, the format can summarize multiple identical \acp{QS}
by allowing several names for each list of placements,
where each name can also have a weight (called its \emph{multiplicity}).
Lastly, usually not all placement locations are stored in the file,
as the ones with low \ac{LWR} do not contribute much to post-analysis methods anyway.

% In summary, the result of a phylogenetic placement analysis is a mapping of the \acp{QS} in a sample
% to positions on the branches of the \ac{RT}.
% Each such position, along with the corresponding \ac{LWR}, is called a placement of the \ac{QS}.
% The result of phylogenetic placement is a mapping of each \ac{QS} (e.g., environmental reads)
% to positions on the branches of the \ac{RT}, amended by probabilities in form of their likelihood weight ratios.
% Each \ac{QS} is thus represented by its placement positions, which are also called its placements.
% along with the respective likelihood weight ratios.f

% Each such sample represents a geographical location, a body site, a point in time, etc.
% In the following, we represent a sample by the placement locations of its metagenomic \acp{QS},
% including the respective per-branch \acp{LWR}.
% Furthermore, for a specific analysis, we assume the standard use case,
% that is, all placements were computed on the same fixed \acf{RT} and \acl{RA}.

% ======================================================================================================================
%     Use Cases and Applications
% ======================================================================================================================

\subsection{Use Cases and Applications}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:UseCasesApplications}

Phylogenetic placement is a flexible tool that yields useful biological information \emph{per se},
but that also can be utilized for a variety of downstream analyses.

\paragraph{Comparison to Existing Methods}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:UseCasesApplications:par:Comparison}

A typical task in metagenomic studies is to identify and classify the environmental sequences with respect
to known reference sequences, either in a taxonomic or phylogenetic context,
as mentioned in \secref{ch:Foundations:sec:SequenceAnalysis:sub:Metagenomics}.
Conventional methods for this task, such as \toolname{BLAST} \cite{Altschul1990}, are based on sequence similarity.
Such methods are fast, but only attain satisfying accuracy levels
if the query sequences are sufficiently similar to the reference sequences.
Furthermore, the best \toolname{BLAST} hit does often \emph{not} represent the most closely related species \cite{Koski2001}.
This is particularly true for environments where available reference databases do not exhibit
sufficient taxon coverage \citep{Mahe2017}.
Moreover, as insufficient taxon coverage cannot be detected by methods that are based on sequence similarity,
they can potentially bias downstream analyses.

More recent methods can alleviate some of these issues, for instance by
using machine learning techniques to obtain a taxonomic classification of metagenomic sequences \cite{Vervier2015},
or by utilizing a phylogeny inferred from metagenomic OTUs to classify microbial communities \cite{Tanaseichuk2014}.
However, the common shortcoming of these methods is
that they lack a way of incorporating phylogenetic information of known sequences.

A phylogenetic placement analysis \emph{does} incorporate known phylogenetic relationships,
and hence provides a more accurate means for read identification and classification.
For example, the classification of query sequences
can be summarized by means of sequence abundances \cite{Pace1997,Hugenholtz1998},
or to obtain taxonomic assignments \cite{Kozlov2016}.

However, phylogenetic placement also allows for more elaborate downstream analyses.
Firstly, the reference tree usually offers a higher resolution than simple per-taxon abundance counts,
and the amount of mapped \acp{QS} per branch can be directly visualized on the \ac{RT} \citep{Mahe2017},
as shown in \secref{ch:Visualization:sec:Motivation}.
Secondly, established methods such as Edge PCA and Squash Clustering \citep{Matsen2011a},
which we introduce in \secref{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods},
allow for identifying subtle differences between distinct samples,
thus enabling comparative studies directly based on phylogenetic placement.
Lastly, we proposed novel methods for visualizing and clustering phylogenetic placement data \citep{Czech2018a},
which we describe in \chpref{ch:Visualization} and \chpref{ch:Clustering}.
Another typical task in metagenomic studies is to discover novel diversity within the samples,
which can be conducted using phylogenetic placement, as outlined in \chpref{ch:ConclusionOutlook}.

\paragraph{Variants and Derived Tools}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:UseCasesApplications:par:DerivedTools}

There also exist variants of phylogenetic placement that use maximum parsimony \cite{Berger2011}
and minimum evolution \cite{Filipski2015} instead of maximum likelihood,
and variants that calculate Bayesian posterior probabilities \cite{Matsen2010}.
The boosting method \toolname{SEPP} has been proposed to improve the accuracy of the placements \cite{Mirarab2012}.
The recently proposed tool \toolname{RAPPAS} \cite{Linard2018} is an alignment-free approach
that is not based on ML, but yields comparable results to standard phylogenetic placement implementations.

Phylogenetic placement has further been used for a variety of applications and derived pipelines, such as
species delimitation as in \toolname{PTP} \cite{Zhang2013} and \toolname{mPTP} \cite{Kapli2017},
genome and metagenome analysis as in \toolname{PhyloSift} \cite{Darling2014},
taxonomic identification and phylogenetic profiling as in \toolname{TIPP} \cite{Nguyen2014}, and
identification and correction of taxonomically mislabeled sequences as in \toolname{Sativa} \cite{Kozlov2016}.

% these tools combine 1,010 more citations (as of 2018-07-01)

% ======================================================================================================================
%     Placement Processing
% ======================================================================================================================

\subsection{Placement Processing}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PlacementProcessing}

% \paragraph{Normalization}
% \label{ch:Foundations:sec:PhylogeneticPlacement:sub:PlacementProcessing:par:Normalization}

When placing multiple environmental samples, for instance, from different geographical locations,
typically, the same \ac{RT} is used, in order to allow for comparisons of the phylogenetic composition of these samples.
In this context, it is important to consider how to properly normalize the samples.
Normalization is required as the sample size (often also called library size),
that is, the number of sequences per sample, can vary by several orders of magnitude,
due to efficiency variations in the sequencing process or biases introduced by the amplification process,
as explained in \secref{ch:Foundations:sec:SequenceAnalysis:sub:Metagenomics}.

Selecting an appropriate normalization strategy hence constitutes a common problem in many metagenomic studies.
The appropriateness depends on data characteristics \cite{Weiss2017}, but also on the biological question asked.
For example, estimating indices such as the species richness are often implemented
via so-called \emph{rarefaction} and rarefaction curves \cite{Gotelli2001}
by randomly re-drawing sequences from the set of sequences in a sample to obtain comparable sample sizes.
This however ignores a potentially large amount of the available valid data \cite{McMurdie2014}.
Furthermore, the specific type of input sequence data has to be taken into account for normalization:
Biases induced by the amplification process can potentially be avoided if, instead of amplicons,
data based on shotgun sequencing are used, such as \textsubscript{mi}tags \cite{Logares2014}.
Moreover, similar sequences can be clustered prior to phylogenetic placement analysis, for instance,
by constructing so-called \emph{operational taxonomic units} (OTUs) \cite{Edgar2010,Mahe2014,Mahe2015,Rognes2016}.
Analyses using OTUs focus on species diversity instead of simple abundances.
OTU clustering substantially reduces the number of sequences,
and hence greatly decreases the computational cost for placement analyses.
Lastly, one may completely ignore the abundances (which correspond to the \emph{multiplicities} of placements)
of the placed sequences, reads, or OTUs, and only be interested in their presence/absence when comparing samples.

Which of the above analysis strategies is deployed,
depends on the specific design of the study and the research question at hand.
The common challenge is that the number of sequences per sample differs, which affects most post-analysis methods.

In the following, we therefore explain how the necessary normalizations of sample sizes can be performed.
We also introduce established terminology,
and describe general techniques for interpreting and working with phylogenetic placement data.
These are not methods of their own, but they are tools and building blocks
that are necessary for the analysis methods explained and introduced later.

\paragraph{Edge Masses}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PlacementProcessing:par:EdgeMasses}

Methods that compare samples directly based on their sequences,
such as the UniFrac distance \cite{Lozupone2005,Lozupone2007a},
can benefit from rarefaction \cite{Weiss2017}.
However, in the context of phylogenetic placement, rarefaction is not necessary.
Thus, more valid data can be kept.
To this end, it is convenient to think of the reference tree as a graph.
% (when exploiting graph properties of the tree, we refer to the branches of the tree as edges).
Then, the per-branch \acp{LWR} for a single \ac{QS}
can be interpreted as mass points distributed over the edges of the \ac{RT},
including their respective placement positions on the branches, as shown in \figref{fig:tree_epa}.
We call this the \emph{mass interpretation} of the placed \acp{QS},
and henceforth use mass and \ac{LWR} interchangeably.
For simplicity, we here ignore that typically not all placements are stored in \fileformat{jplace} files,
meaning that the mass per QS can also be $<1$.
Hence, each \ac{QS} is assumed to have a total accumulated mass of $1.0$ on the \ac{RT}.
The \emph{mass of an edge} or \emph{edge mass} refers to the sum of the \acp{LWR} on that edge for all \acp{QS} of a sample,
as shown in \figref{fig:imbalance:sub:ReferenceTree}.
The \emph{total mass} of a sample is then the sum over all edge masses,
which is identical to the number of \acp{QS} in the sample,

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{imbalance.pdf}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:imbalance:sub:ReferenceTree}
    \end{subfigure}
    \begin{subfigure}{0pt}
        \phantomcaption
        \label{fig:imbalance:sub:Matrices}
    \end{subfigure}
    \caption[Edge Masses and Imbalances]{
        \textbf{Edge Masses and Imbalances.}
        \subref{fig:imbalance:sub:ReferenceTree}
        Reference tree where each edge is annotated with the normalized mass (first value, blue) and
        imbalance (second value, red) of the placements in a sample.
        The imbalance is the sum of masses on the root side of the edge minus the sum of the masses on the non-root side.
        The depicted tree is unrooted, hence, its top-level trifurcation (gray dot) is used as ``root'' node.
        An exemplary calculation of the imbalance is given at the top.
        Because terminal edges only have a root side, their imbalance is not informative.
        \subref{fig:imbalance:sub:Matrices}
        The masses and imbalances for the edges of a sample constitute the rows of the first two matrices.
        The third matrix contains the available meta-data features for each sample.
        These matrices are used to calculate, for instance, the edge principal components or correlation coefficients.
    }
    \label{fig:imbalance}
\end{figure}

The key idea is to use the distribution of placement mass points over the edges of the \ac{RT} to characterize a sample.
This allows for normalizing samples of different size
by scaling the total sample mass to unit mass $1.0$.
In other words, absolute abundances are converted into relative abundances.
This way, rare species, which might have been removed by rarefaction, can be kept,
as they only contribute a negligible mass to the branches into which they have been placed.
This approach is analogous to using proportional values for methods based on OTU count tables,
that is, scaling each sample/column of the table by its sum of OTU counts \cite{Weiss2017}.
Most of the methods presented here use normalized samples, that is, they use relative abundances.
As relative abundances are compositional data, certain caveats occur \cite{Aitchison1986,Lovell2015,Gloor2016},
which we discuss where appropriate.

When working with large numbers of \acp{QS},
the mass interpretation allows to further simplify and reduce the data:
The masses on each edge of the tree can be quantized into $b$ discrete bins,
that is, each edge is divided into $b$ intervals (or bins) of the corresponding branch length.
All mass points on that edge are then accumulated into their respective nearest bin.
For example, by accumulating mass points at their nearest interval midpoint, masses are only minimally moved.
The parameter $b$ controls the resolution and accuracy of this approximation.
In the extreme case of $b:=1$, all masses on an edge are grouped into one single bin.
This \emph{branch binning} process drastically reduces the number of mass points
that need to be stored and analyzed in several methods we present,
while only inducing a negligible decrease in accuracy.
As shown in \tabref{tab:hmp_binning_error}.
branch binning can yield a speedup of up to 75\% for post-analysis run-times.

Furthermore, using masses allows to summarize a set of samples
by annotating the \ac{RT} with their average per-edge mass distribution.
This procedure, also called \emph{squashing} \cite{Matsen2011a}, sums over all sample masses per edge
and then normalizes them once more to obtain unit mass for this resulting average tree.
This normalized tree thereby summarizes the (sub-)set of samples it represents.

\paragraph{Edge Imbalances}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PlacementProcessing:par:EdgeImbalances}

So far, we have only considered the per-edge masses.
Often, however, it is also of interest to ``summarize'' the mass of an entire clade, that is, to consider per-clade masses.
For example, sequences of the \ac{RT} that represent species or strains might not provide sufficient phylogenetic signal
for properly resolving the phylogenetic placement of short sequences \cite{Dunthorn2014}.
In these cases, the placement mass of a sequence can be spread across different edges representing the same genus or species,
thus blurring analyses based on per-edge masses.

Instead, a clade-based summary can yield clearer analysis results.
It can be computed by using the tree structure to appropriately transform the edge masses.
Each edge splits the tree into two parts
(bipartitions, see \secref{ch:Foundations:sec:TreeOfLife:sub:PhylogeneticTrees:par:TreeProperties}),
of which only one contains the root (or top-level trifurcation) of the tree.
For a given edge, its mass difference is then calculated by summing all masses in the root part of the tree
and subtracting all masses in the other part,
while ignoring the mass of the edge itself \cite{Matsen2011a}.
This difference is called the \emph{imbalance} of the edge.
It is usually normalized to represent unit total mass,
as the absolute (not normalized) imbalance otherwise propagates the effects of differing sample sizes all across the tree.
It is irrelevant where the root of the tree is,
as any re-rooting changes the sign of edge imbalance values consistently across different samples.

An example of the imbalance calculation is shown in \figref{fig:imbalance:sub:ReferenceTree}.
The edge imbalance relates the masses on the two sides of an edge to each other.
This implicitly captures the \ac{RT} topology and reveals information about its clades.
Furthermore, this transformation can also reveal differences in the placement mass distribution
of nearby branches of the tree.
This is in contrast to the KR distance
(see \secref{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods:par:Distances}),
which yields low values for masses that are close to each other on the tree.
Note that for normalized samples with unit total mass,
the imbalance of a leaf edge is simply the total mass of the tree minus the mass of the edge.
It thus contains mostly irrelevant information and can often be left out.

\paragraph{Placement Data Matrices}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:PlacementProcessing:par:PlacementDataMatrices}

The edge masses and edge imbalances per sample can be summarized by two matrices,
which we use for all further downstream edge- and clade-related analyses, respectively.
In these matrices, each row corresponds to a sample, and each column to an edge of the \ac{RT}.
Note that these matrices can either store absolute or relative abundances,
depending on whether the placement mass was normalized.

Furthermore, many studies provide meta-data for their samples,
for instance, the pH value or temperature of the samples' environment.
Such meta-data features can also be summarized in a per-sample matrix, where each column corresponds to one feature.
The three matrices are shown in \figref{fig:imbalance:sub:Matrices}.
Quantitative meta-data features are the most suitable for computational purposes,
as they can be used to detect correlations with the placement mass distributions of samples.

% ======================================================================================================================
%     Existing Analysis Methods
% ======================================================================================================================

\subsection{Existing Analysis Methods}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods}

\paragraph{Distance between Samples}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods:par:Distances}

motivate: moving along the branches means distance between things.

kr distance. ref to nh distance?!
\cite{Evans2012}

\paragraph{Squash Clustering}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods:par:SquashClustering}

squash clustering
\cite{Matsen2011a,Evans2012}


\paragraph{Edge PCA}
\label{ch:Foundations:sec:PhylogeneticPlacement:sub:ExistingMethods:par:EdgePCA}

For example, Edge principal components analysis (Edge PCA) \cite{Matsen2011a}
is a method that utilizes the imbalance matrix to detect and visualize edges
with a high heterogeneity of mass difference between samples.
Edge PCA further allows to annotate its plots with meta-data variables, for instance, by coloring,
thus establishing a connection between differences in samples and differences in their meta-data \cite{Srinivasan2012}.

edge pca
\cite{Matsen2011a,Evans2012}

we use these methods repeatedly to compare our novel methods to.
